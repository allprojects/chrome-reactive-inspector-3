\chapter{Evaluation}
In this chapter we evaluate the contributions of this thesis and the general state of the project. On that account we first examine the changes to the UI, the Source Code Previews and performance improvements. We then verify the most important features of CRI by checking a set of specifications for each test application and discuss the result.

All performance measurements and tests in this chapter were performed with Chrome Version 64.0.3282.140 (Offizieller Build) (64-Bit), WebStorm 2017 3.4 and Windows 10 Home 64 bit version 1709 build 16299.192. The used device has 8GB RAM, Intel i7 7500 2.7GHz 4CPUs Processor with integrated Intel(R) HD Graphics 620. Each test application was hosted on the integrated Web server of WebStorm on the local machine. No other applications were running, but background processes and services as well as other unpredictable factors have to be taken into account that may have influenced the results of the performance measurements.

\section{Evaluation of Improvements}
This section examines the improvements introduced by this thesis and their implementations. 

	\subsection{Changes to the User Interface}
	Most of the changes to the User Interface are, by their nature, hard to evaluate in regards to their usability improvement. The experienced usability of a User Interface can be highly subjective \cite, the easiest way to achieve consistent evaluation results is therefore by collecting and analyzing empirical data, for example through Click stream analysis \cite{Clickstream}. Since CRI is not yet used in active production systems by a large user base, a User Study would be the usual approach. Unfortunately this is outside the scope of this thesis. However, some changes can be verified as improvements. For example, the text per node in the Dependency Graph was reduced without loosing any information can be seen as a general improvement of usability, because the change did not affect any other part of the UI and removed obsolete text. Another change that can be objectively categorized as an improvement is that parts of the UI which are not necessary to examine the Dependency Graph can now be hidden. Although it has to be noted that this change added a new UI element - the \emph{hide} button - which could potentially introduces new usability issues. Highlighting of the currently updated edge in addition to nodes also provides additional information for the user, to help track down differences between steps that were not as easily detectable before, without affecting other aspects of the UI.

	\subsection{Inspecting Source Code Tooltips}
	The Source Code Tooltips that connect nodes in the Dependency Graph with the JS code they originate from are created whenever the user hovers over a node and presses the CTRL key. Due to their on-demand nature, their performance impact on the time critical computations of CRI like the recording process can be neglected. The original JS files are stored in a variable in a Content script of CRI that are queried for the lines corresponding to a node if requested. Storing a copy of the whole JS file in addition to the instrumented version however should not exceed the Memory (RAM) of any modern computer that can run Google Chrome. We will examine the robustness of this feature in regards to its accuracy in a later section as part of the test application evaluation. To investigate and verify the usefulness of Source Code Tooltips we examined each test application and calculated the percentage of nodes that gain additional context through this feature. As a \emph{named} node already has some form of context that helps the user to identify it and its dependents, we counted \emph{named} nodes separately. For the exact circumstances and inputs used for each test application see section \ref{sec:EvalTests}. Over all test applications there were 581 nodes. Of these, 436 nodes had a Source Code Tooltip which equals to approximately 75\% of all nodes. Only 197, approximately 33.9\% of all nodes are \emph{named}. This means that the introduction of Source Code Tooltips provided additional means to identify a node to the user for 35.1\% of the total number of nodes. The remaining 145 nodes that can not yet be linked to specific positions in the source code in part consist of nodes that can be easily interpreted by examining the nodes that depend on them. For example in the test application "RXJS /Canvas Painting" %TODO test ref
	the node with Id 6 is a FromEventObservable created by "Rx.Observable.fromEvent(colorchar,"click")" and is not yet detected by the Jalangi analysis.

	\subsection{Scrutinizing performance with rapidly updated Observables}
	\label{sec:PerformanceEvaluation}
	In this section we evaluate the performance improvements resulting from the reworked graph history and the less frequent UI rendering in detail. To this purpose we developed a new test application for RxJS which can be used as a simple benchmark on how well a CRI, and in the last part of this section RxFiddle, can cope with a simple rapidly updating observable. The application uses an Interval Observable to generate one update every five milliseconds over a period of five seconds. In listing \ref{lst:performanceTestExtract} the Observable responsible for the updates is shown. The full source code with the termination after five seconds is in the appendix. %TODO appendix 
	
	\begin{lstlisting}[language=JavaScript, caption={Example of RxJS code.},label={lst:performanceTestExtract}]
	var intervalObservable = Rx.Observable.interval(5)
	.timestamp()
	.bufferCount(2, 1)
	.map(function (w) {
	return w[1].timestamp - w[0].timestamp;
	})
	.share();	
	\end{lstlisting}

	To exclude the initial setup of CRI from the gathered performance data we create a Reactive Breakpoint for the \emph{nodeCreated} event of node one ("nodeCreated[1]"), ran the application until it paused at the Reactive Breakpoint, started the performance recording and then continued execution. To record a CPU profile that includes the time spent per function we used the Chrome-JavaScript Profiler. To inspect the memory usage we used the Memory tab of Chrome DevTools. The duration of the recordings vary from the actual time the execution of the test application took and its recording took, because they were started and ended manually. 
	
	\textbf{CPU Profile}
	% pic of CPU profile CRI2 besides CRI3
	
	The label "(program)" in the recording entitles native code execution of Chrome. The measured execution time for this label is far less accurate than any other measurement, because the individual composition is unclear and can solely increase by stopping the recording at a later point in time due to the limitations of the recording tool.
	
	For CRI2 the execution of PerformanceTest took approximately 154.0 seconds with an overall recording interval of 187.9 seconds. To differentiate between execution time and recording time we selected an area with significantly higher CPU load that should correspond reasonably well to the actual execution time. The largest percentage of \emph{Self Time} (the time spent executing code directly in the function) spent in a single function directly correspond to excessive UI updates - which happen at least once per created step since the slider is updated and triggers the rendering of a new Dependency Graph for that step. \emph{getBB} (getBoundingBox) calculates the size of nodes in the graph, whereas  \emph{buildFragment}'s impact stems form the \emph{domManip} function of jQuery.
	For CRI3 the execution took approximately 5.5 seconds with an overall recording interval of 23.2 seconds. It can be observed that still a sizable amount of time is spent inside UI related computations especially \emph{getBB}. The most time of execution not related to Chromes native code is spent inside the \emph{ja} function of jQuery that can not easily be tracked down to CRI code. Overall the most time is spent inside Chromes native code, but as described earlier this can have numerous reasons. Due to the nature of changes from CRI2 to CRI3 we can however account at least part of that execution time to storage operations using the Chrome Storage API. 
	As CRI3 was approximately 28 times faster than CRI2, the performance improvements regarding rapid updates implemented by this thesis were effective. It is however worth noting, that CRI2 can barely handle this test without crashing - the UI becomes temporarily unresponsive. This probably increases the performance differences between the two versions of CRI more than it would be for a test both CRI versions can handle well (without the UI becoming temporarily unresponsive). We chose to keep the test as-is to underline the impact of \emph{async} buildup and load exceeding the extension's capacity. 
	As of now CRI3 also has a fixed capacity of updates it can handle without crashing, but it is much higher than CRI2's mostly due to the throttling of rendering operations. Basic tools to detect and react to an application exceeding that capacity are already in place for CRI3 and could be easily extended to automatically detect if rendering computations accumulate beyond its limit and increase or decrease the throttle interval for UI updates on demand. Since all steps are recorded even if the rendering can not keep up, increasing the throttle time to keep the extension from crashing is a viable approach. The user can then pause the recording anytime and examine the steps in details whereas a crash will render CRI useless to the user.
	
	% - memory test for CRI2 had to be restarted several times before it would complete the execution without crashing . Few forced UI updates by dragging the Dependency Graph around a few centimeters.
	%RAM::
	% CRI2 - RAM: Total RAM approx 52.7MB
	% CRI3 -RAM: Total RAM approx 14.6MB
	% most notable difference in a category is the size of JS array which includes the most part of the stored Dependency Graph
	% Comparison to same Test app with pause event after 250ms instead of 5s:
	% CRI2: ~16.1MB -> increases rapidly over time
	% CRI3: ~12.6MB -> increases very little over time
	% for 250ms both version have a fairly similar Memory consumption, although CRI2 has a noticable delay before the Dependency Graph is shown (still <5s) while CRI3 has no noticeable delay.
	Since CRI3's History implements a stream like approach through paging, the memory usage once the size of a single >page< is exceeded is fairly constant and does not increase with the number of steps. As CRI2's History does not support paging the memory usage increases approximately proportional to the number of steps in the History. However since modern computers have usually more than 1GB of Memory, even the higher Memory consumption of CRI2 does not have any influence on the performance detectable by the user.

	
	\textbf{Comparison to RxFiddle}
	We also examined the performance of RxFiddle with the used test application and compared it to CRI3 to qualify the discussion in section \ref{sec:RxFiddle} on its performance with rapidly updated Observables.
	% Comparison for test case PerformanceTest: CRI vs RxFiddle
		% although RxFiddle does not have performance issues (lags > 1 s when hovering over a node) for ~ 1000 updates on Chrome. It does block Firefox (57.0.4 (64-bit)) for a few seconds. Once the UI is build it is noticeable slower than for applications with less value changes but delays < 1s. The CRI performance for this test app is slightly faster, but due to it being an extension instead of a webpage the performance difference may not be due to the tool implementations. CRI generates ~ 2700 steps, RxFiddle generates ~ 1000 values for each operator/observable in the chain of "intervalObservable".
		% CRI is not testable in Firefox 
	
\section{Reviewing the Test Applications}
\label{sec:EvalTests}
To establish the current state of the most important features of CRI we compiled a set of specifications that we validated for each of the test applications. They were also designed to provide a baseline for the robustness of the current CRI implementation. Note that we also included specifications targeting features not introduced by this thesis as a means of manual regression testing for the most important aspects of CRI. Since most of these specifications can not be tested for each node, step and/or case in reasonable time, we specified the exact tests that were performed in order to approximate the results. These test probes should provide a reasonably precise evaluation and ,in contrast to some of the specifications, can easily be reproduced to verify our results.

\textbf{Specifications}
\begin{itemize}
	\item The dependency graph is shown and all observables that are assigned to a named variable are displayed distinctively. (Test: Up to the first five named observables in the code are all displayed with an orange background.)
	\item The history of the dependency graph can be navigated. It is possible to navigate to the previous, succeeding or a random step in the history. (Test: Jump to the median step; click next five times; click previous five times.)
	\item For each node, no space is occupied by fields that hold no value in neither the node itself nor it's tooltip.
	\item The ids of the nodes start at one and are continuous. If the test application is started again with the exact same inputs, each node has the same id as in the last execution.
	\item The Source Code Tooltips show and highlight the corresponding lines of code for each node that has one. (Test: If possible, choose two nodes, one that corresponds to the middle of a chain of reactive function calls and one that corresponds to the end of a chain of reactive function calls. For both nodes check if the highlighting is correct.)
	\item The node or edge that was updated in a step is highlighted. (Test: Check this behavior for the first ten steps in the history.)
	\item The history queries can be used to search for a specific event in the history. (Test: Queries for \emph{evaluationYielded} and \emph{nodeUpdated} find the corresponding steps for the first named node.)
	\item The graph can be searched, the matching node(s) is/are highlighted and if the search is reseted the original coloring is restored. (Test: Search for the dependencies and dependents of the first named node and then reset the search.)
	\item Reactive breakpoints can be used to pause the debugger when a specific event occurs. (Test: A breakpoint set for the first node created will break at step one. A breakpoint for the first node updated will break before the value is updated in the original observable. Note: The value will be already updated in the CRI UI.)
\end{itemize}

	Results in abstract - Detailed result tables in appendix.
	% - short mention of excluded tests:
		% RxJS/all-operators - artifical examples by Pradeep Baradur used to verify operators but not a suitable test application.
		% RxJS/spotify-search - great test application in general, but the used API does not work without a token.
	% findings:
	% - statistical results: For this statistic spec 9 was interpreted as two different results to have a simple result with Yes and No's - Ambiguity was counted as No because it means not fully supported yet. N.A. was counted as Yes because it does no additional work to be done by a dev
		% verified 388/420 working
		% specs 2,3,4,5,6,9.2 are without errors in any application
	% Exceptions in detail:
	% - N.A. mostly not applicable because there were no named nodes
	% - different classes of errors or details of a single error AND if they were present in CRI2: 
		% - Ambiguity: CRI can not handle nodes with the same name. e.g. a named node created in a for loop. CRI will use the first or last node depending on the context and is not equipped to handle ambiguous names consitently yet. Dependencies can not be found correctly, history queries will only match one node. # verified CRI2 with split file test
		% - spec 9 : N*,Y - the nodes are added out of order in RxJS if the first node does not stem from jalangi analysis. #verified CRI2 with son_father_wallet
		% - Dependencies or Dependents not correctly highlighted: seems to be related to dynamically created nodes, but not always. Is a bug. # different behavior with CRI2 crop example - new bug, equal behavior with stopwatch example.
		% - alphabetinvasion: observables are not correctly recorded. The test uses closures and custom operators (although custom operators do work in RxJS\animationtest). Example observable that is missing!!! # verified with CRI2
		% - BaconJS/blog-url the named variable is not correctly detected, futhermore the bacon recording does not detect the correct observables. # verified with CRI2
	% example mario: still to fast to keep up - will crash if not paused
	% -> in case of mario - hard to detect the real reason due to overload of Chrome itself - the async requests on communication api seems to build up because they are to many for Chrome to keep up.
	% --> crashes CRI Chrome extension if not paused
	
	%  They should be the target of further development.